{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwXorfug2ZK5iVPZ72PxrM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Regression"],"metadata":{"id":"8GFTGFzBP-Hh"}},{"cell_type":"markdown","source":["#1. What is Simple Linear Regression?\n","  - Simple Linear Regression is a statistical method that models the relationship between two continuous variables: one independent variable (X) and one dependent variable (Y), using a straight-line equation of the form Y = mX + c.\n","\n","#2. What are the key assumptions of Simple Linear Regression?\n","  - Linearity: Relationship between X and Y is linear.\n","  - Independence: Observations are independent.\n","  - Homoscedasticity: Constant variance of residuals.\n","  - Normality: Residuals are normally distributed.\n","\n","#3. What does the coefficient m represent in the equation Y=mX+c?\n","  - The coefficient m (slope) indicates the change in Y for a one-unit change in X.\n","\n","#4. What does the intercept c represent in the equation Y=mX+c?\n","  - The intercept c is the predicted value of Y when X = 0.\n","\n","#5. How do we calculate the slope m in Simple Linear Regression?\n","  - m= ∑[(Xᵢ − X̄)(Yᵢ − Ȳ)]/ Σ[(Xᵢ − X̄)²]\n","\n","#6. What is the purpose of the least squares method in Simple Linear Regression?\n","  - It minimizes the sum of the squares of the residuals (errors) to find the best-fitting line.\n","\n","#7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n","  - R² represents the proportion of variance in Y explained by X. Ranges from 0 to 1.\n","\n","#8. What is Multiple Linear Regression?\n","  - A regression method that models the relationship between one dependent variable and two or more independent variables.\n","\n","#9. What is the main difference between Simple and Multiple Linear Regression?\n","  - Simple Linear Regression uses one independent variable; Multiple Linear Regression uses multiple independent variables.\n","\n","#10. What are the key assumptions of Multiple Linear Regression?\n","   - Linearity\n","   - Independence\n","   - Homoscedasticity\n","   - Normality of residuals\n","   - No multicollinearity\n","\n","#11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n","   - Heteroscedasticity occurs when residuals have unequal variance. It affects the accuracy of standard errors and hypothesis tests.\n","\n","#12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n","   - Remove correlated predictors\n","   - Use Principal Component Analysis (PCA)\n","   - Apply regularization (Ridge, Lasso)\n","\n","#13. What are some common techniques for transforming categorical variables for use in regression models?\n","   - One-hot encoding\n","   - Label encoding\n","   - Binary encoding\n","   - Ordinal encoding (if categories have order)\n","\n","#14. What is the role of interaction terms in Multiple Linear Regression?\n","   - They capture the combined effect of two variables when the effect of one variable depends on the level of another.\n","\n","#15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n","   - In Simple Regression, the intercept is the value of Y when X = 0.\n","In Multiple Regression, it's the value of Y when all X variables are 0 (which may not be meaningful if zero isn't in the range of the predictors).\n","\n","#16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n","   - The slope shows the direction and strength of the relationship. A higher absolute value indicates a stronger effect on Y.\n","\n","#17. How does the intercept in a regression model provide context for the relationship between variables?\n","   - It serves as a baseline value of the dependent variable when all independent variables are zero.\n","\n","#18. What are the limitations of using R² as a sole measure of model performance?\n","   - Doesn't indicate if model is appropriate\n","   - Increases with more variables, even irrelevant ones\n","   - Doesn't measure overfitting or prediction accuracy\n","\n","#19. How would you interpret a large standard error for a regression coefficient?\n","   - It implies high uncertainty in the estimate, suggesting the coefficient may not be statistically significant.\n","\n","#20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n","   - Residuals show a funnel or fan shape. It's important to fix because it can lead to inefficient estimates and biased statistical tests.\n","\n","#21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n","   - It suggests that added variables don't contribute much predictive power and may be unnecessary.\n","\n","#22. Why is it important to scale variables in Multiple Linear Regression?\n","   - Ensures coefficients are comparable\n","   - Helps with convergence in optimization\n","   - Reduces multicollinearity in regularized models\n","\n","#23. What is polynomial regression?\n","   - A type of regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial.\n","\n","#24. How does polynomial regression differ from linear regression?\n","   - Polynomial regression fits a curved line, while linear regression fits a straight line.\n","\n","#25. When is polynomial regression used?\n","   - When the data shows a non-linear relationship that a straight line can't capture.\n","\n","#26. What is the general equation for polynomial regression?\n","   - Y=b₀+b₁X+b₂X²+b₃X³+⋯+bₙXⁿ\n","\n","#27. Can polynomial regression be applied to multiple variables?\n","   - Yes, known as Multivariate Polynomial Regression where interaction and power terms of multiple variables are included.\n","\n","#28. What are the limitations of polynomial regression?\n","   - Prone to overfitting\n","   - Sensitive to outliers\n","   - Can become complex with high degrees\n","\n","#29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n","   - Cross-validation\n","   - Adjusted R²\n","   - AIC/BIC\n","   - Visual inspection of residuals\n","\n","#30. Why is visualization important in polynomial regression?\n","   - It helps understand the model's fit, detect overfitting, and interpret relationships in curved patterns.\n","\n","#31. How is polynomial regression implemented in Python?\n","              - from sklearn.linear_model import LinearRegression\n","                from sklearn.preprocessing import PolynomialFeatures\n","                X = [[1], [2], [3]]\n","                y = [2, 6, 14]\n","                poly = PolynomialFeatures(degree=2)\n","                X_poly = poly.fit_transform(X)\n","                model = LinearRegression()\n","                model.fit(X_poly, y)\n"],"metadata":{"id":"HqnKpVbVQBRr"}},{"cell_type":"code","source":[],"metadata":{"id":"RCLwJ9QsQDu8"},"execution_count":null,"outputs":[]}]}